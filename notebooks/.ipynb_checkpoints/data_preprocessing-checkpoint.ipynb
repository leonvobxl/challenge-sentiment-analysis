{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095cee08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Import datasets\n",
    "print(\"Loading data\")\n",
    "df_train = pd.read_csv('data/train.csv')\n",
    "df_test = pd.read_csv('data/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72be0a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define processing methods\n",
    "wordNetLemmatizer = WordNetLemmatizer()\n",
    "porterStemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d1c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process tweets\n",
    "def clean_tweet(data, wordNetLemmatizer, porterStemmer):\n",
    "    data['clean_twt'] = data['tweet']\n",
    "    data['clean_twt'] = data['clean_twt'].str.replace(\"@[\\w]*\",\"\") #Removing user handles starting with @\n",
    "    data['clean_twt'] = data['clean_twt'].str.replace(\"[^a-zA-Z' ]\",\"\")    #Removing numbers and special characters    \n",
    "    data['clean_twt'] = data['clean_twt'].replace(re.compile(r\"((www\\.[^\\s]+)|(https?://[^\\s]+))\"), \"\")    #Removing ur \n",
    "    data['clean_twt'] = data['clean_twt'].replace(re.compile(r\"(^| ).( |$)\"), \" \")    #Removing single characters\n",
    "    \n",
    "    #Tokenizing\n",
    "    data['clean_twt'] = data['clean_twt'].str.split()\n",
    "    data['clean_twt'] = data['clean_twt'].apply(lambda tweet: [word for word in tweet if word not in STOPWORDS])     #Removing stopwords\n",
    "    data['clean_twt'] = data['clean_twt'].apply(lambda tweet: expand_tweet(tweet))     #Expanding not words\n",
    "    data['clean_twt'] = data['clean_twt'].apply(lambda tweet: [wordNetLemmatizer.lemmatize(word) for word in tweet])     #Lemmatizing the words    \n",
    "    data['clean_twt'] = data['clean_twt'].apply(lambda tweet: [porterStemmer.stem(word) for word in tweet])     #Stemming the words   \n",
    "    data['clean_twt'] = data['clean_twt'].apply(lambda tweet: ' '.join(tweet))    #Combining words back to tweets\"\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c540572",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256314c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing the tweets\n",
    "#Processing training data\n",
    "df_train = clean_tweet(df_train, wordNetLemmatizer, porterStemmer)\n",
    "\n",
    "#Train data processed and saved to data/clean_train.csv\n",
    "df_train.to_csv('data/clean_train.csv', index = False)\n",
    "\n",
    "#Processing test data\n",
    "df_test = clean_tweet(df_test, wordNetLemmatizer, porterStemmer)\n",
    "\n",
    "#Test data processed and saved to data/clean_test.csv\n",
    "df_test.to_csv('data/clean_test.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9347e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
